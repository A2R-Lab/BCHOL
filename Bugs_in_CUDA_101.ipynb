{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvLfjbWNLEMucPkHT+mu8n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A2R-Lab/rsLQR-Spring23/blob/main/Bugs_in_CUDA_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3XRf5Ntss-P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Don't forget to make sure your collab notebook runs on GPU ! Go to settings and select GPU in hardware accelerator.\n",
        "2. When you move any object from RAM to shared don't forget to move it back to RAM after calling the device function.\n",
        "3. You can use the following skeleton for CUDA implementations"
      ],
      "metadata": {
        "id": "7CFIcd6jsvRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda -n run.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <iostream>\n",
        "#include <cmath>\n",
        "\n",
        "//DEVICE FUNCTIONS\n",
        "\n",
        "template <typename T> \n",
        "__device__ \n",
        "void funct_cu(T *s_A) {\n",
        "    //do something\n",
        "    __syncthreads();\n",
        "}\n",
        "\n",
        "\n",
        "//GLOBAL/Kernel function\n",
        "\n",
        "template <typename T>\n",
        "__global__ \n",
        "void funct_Kernel(T *d_A) {\n",
        "    \n",
        "    //declare shared memory\n",
        "    __shared__ T s_A[9];\n",
        "\n",
        "    // move RAM memory to shared\n",
        "    for(unsigned i = threadIdx.x; i < n*n; i += blockDim.x){s_A[i] = d_A[i];}\n",
        "\n",
        "    // call device function\n",
        "    funct_cu<T>(s_A);\n",
        "\n",
        "    // move shared to RAM\n",
        "    for(unsigned i = threadIdx.x; i < n*n; i += blockDim.x){d_A[i] = s_A[i];}\n",
        "}\n",
        "\n",
        "\n",
        "//main/host function\n",
        "\n",
        "__host__\n",
        "int main() {\n",
        "    // Input matrix on the host\n",
        "    int N=3;\n",
        "    float A[N*N] = {6,15,55,15,55,225,55,225,979};\n",
        "\n",
        "\n",
        "    // Allocate memory on the GPU for the input and output matrices\n",
        "    float* d_A; cudaMalloc((void**)&d_A, N * N * sizeof(float));\n",
        "\n",
        "    // Copy the input matrix from the host to the GPU memory\n",
        "    cudaMemcpy(d_A, A, N * N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    //Launch the CUDA kernel with appropriate block and grid dimensions\n",
        "    int blockSize = 256;\n",
        "    int gridSize = 1;\n",
        "    funct_Kernel<float><<<gridSize, blockSize>>>(d_A);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "\n",
        "    // Copy the result back from the GPU memory to the host\n",
        "    cudaMemcpy(A, d_A, N * N * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "    \n",
        "    // Free the allocated GPU memory\n",
        "    cudaFree(d_A);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "fEKm3ZKktLVA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}